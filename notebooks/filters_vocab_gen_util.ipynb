{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##Inspired from [@ggerganov](https://github.com/ggerganov/whisper.cpp/blob/6f82320b053bb8183a1734e09c940d6bf2a0f4b2/models/convert-pt-to-ggml.py) "
      ],
      "metadata": {
        "id": "qPWqt6hU9AbT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Clone whisper repository"
      ],
      "metadata": {
        "id": "J8_qz3GD9S3T"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kzziH8swyL6Z",
        "outputId": "e561fbda-f5e3-4ddb-c56b-565b3efc56a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'whisper' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/openai/whisper.git",
        "!cd whisper; git checkout `git rev-list -n 1 --first-parent --before=\"2022-09-23 13:37\" main`"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Download tiny openai/whisper model "
      ],
      "metadata": {
        "id": "bA2_mSFP9Wo7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://openaipublic.azureedge.net/main/whisper/models/65147644a518d12f04e32d6f3b26facc3f8dd46e5390956a9424a650c0ce22b9/tiny.pt\n",
        "!wget https://openaipublic.azureedge.net/main/whisper/models/d3dd57d32accea0b295c96e26691aa14d8822fac7d9d27d5dc00b4ca2826dd03/tiny.en.pt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x60j4DTT2zs9",
        "outputId": "d3553fe1-12db-4966-8421-ecb0cc6610cf"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-10-28 20:57:23--  https://openaipublic.azureedge.net/main/whisper/models/65147644a518d12f04e32d6f3b26facc3f8dd46e5390956a9424a650c0ce22b9/tiny.pt\n",
            "Resolving openaipublic.azureedge.net (openaipublic.azureedge.net)... 13.107.246.18, 2620:1ec:bdf::70, 2620:1ec:46::70\n",
            "Connecting to openaipublic.azureedge.net (openaipublic.azureedge.net)|13.107.246.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 75572083 (72M) [application/octet-stream]\n",
            "Saving to: ‘tiny.pt.1’\n",
            "\n",
            "tiny.pt.1           100%[===================>]  72.07M  60.5MB/s    in 1.2s    \n",
            "\n",
            "2022-10-28 20:57:24 (60.5 MB/s) - ‘tiny.pt.1’ saved [75572083/75572083]\n",
            "\n",
            "--2022-10-28 20:57:24--  https://openaipublic.azureedge.net/main/whisper/models/d3dd57d32accea0b295c96e26691aa14d8822fac7d9d27d5dc00b4ca2826dd03/tiny.en.pt\n",
            "Resolving openaipublic.azureedge.net (openaipublic.azureedge.net)... 13.107.246.18, 2620:1ec:bdf::70\n",
            "Connecting to openaipublic.azureedge.net (openaipublic.azureedge.net)|13.107.246.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 75571315 (72M) [application/octet-stream]\n",
            "Saving to: ‘tiny.en.pt.1’\n",
            "\n",
            "tiny.en.pt.1        100%[===================>]  72.07M  38.8MB/s    in 1.9s    \n",
            "\n",
            "2022-10-28 20:57:26 (38.8 MB/s) - ‘tiny.en.pt.1’ saved [75571315/75571315]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Build tokenzer"
      ],
      "metadata": {
        "id": "yJPzCkef9fX_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "from transformers import GPTJForCausalLM\n",
        "from transformers import GPT2TokenizerFast\n",
        "\n",
        "# ref: https://github.com/openai/whisper/blob/8cf36f3508c9acd341a45eb2364239a3d81458b9/whisper/tokenizer.py#L273-L292\n",
        "def build_tokenizer(path_to_whisper_repo: str, name: str = \"gpt2\"):\n",
        "    os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "    path = os.path.join(path_to_whisper_repo, \"whisper/assets\", name)\n",
        "    tokenizer = GPT2TokenizerFast.from_pretrained(path)\n",
        "\n",
        "    specials = [\n",
        "        \"<|startoftranscript|>\",\n",
        "        *[f\"<|{lang}|>\" for lang in LANGUAGES.keys()],\n",
        "        \"<|translate|>\",\n",
        "        \"<|transcribe|>\",\n",
        "        \"<|startoflm|>\",\n",
        "        \"<|startofprev|>\",\n",
        "        \"<|nocaptions|>\",\n",
        "        \"<|notimestamps|>\",\n",
        "    ]\n",
        "\n",
        "    tokenizer.add_special_tokens(dict(additional_special_tokens=specials))\n",
        "    return tokenizer"
      ],
      "metadata": {
        "id": "s5XoZR774zDy",
        "outputId": "d742c5c3-e2f2-45f4-adab-d75f9b4f6e7d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.23.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.13.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.13.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.9.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.9.24)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ref: https://github.com/openai/whisper/blob/8cf36f3508c9acd341a45eb2364239a3d81458b9/whisper/tokenizer.py#L10-L110\n",
        "LANGUAGES = {\n",
        "    \"en\": \"english\",\n",
        "    \"zh\": \"chinese\",\n",
        "    \"de\": \"german\",\n",
        "    \"es\": \"spanish\",\n",
        "    \"ru\": \"russian\",\n",
        "    \"ko\": \"korean\",\n",
        "    \"fr\": \"french\",\n",
        "    \"ja\": \"japanese\",\n",
        "    \"pt\": \"portuguese\",\n",
        "    \"tr\": \"turkish\",\n",
        "    \"pl\": \"polish\",\n",
        "    \"ca\": \"catalan\",\n",
        "    \"nl\": \"dutch\",\n",
        "    \"ar\": \"arabic\",\n",
        "    \"sv\": \"swedish\",\n",
        "    \"it\": \"italian\",\n",
        "    \"id\": \"indonesian\",\n",
        "    \"hi\": \"hindi\",\n",
        "    \"fi\": \"finnish\",\n",
        "    \"vi\": \"vietnamese\",\n",
        "    \"iw\": \"hebrew\",\n",
        "    \"uk\": \"ukrainian\",\n",
        "    \"el\": \"greek\",\n",
        "    \"ms\": \"malay\",\n",
        "    \"cs\": \"czech\",\n",
        "    \"ro\": \"romanian\",\n",
        "    \"da\": \"danish\",\n",
        "    \"hu\": \"hungarian\",\n",
        "    \"ta\": \"tamil\",\n",
        "    \"no\": \"norwegian\",\n",
        "    \"th\": \"thai\",\n",
        "    \"ur\": \"urdu\",\n",
        "    \"hr\": \"croatian\",\n",
        "    \"bg\": \"bulgarian\",\n",
        "    \"lt\": \"lithuanian\",\n",
        "    \"la\": \"latin\",\n",
        "    \"mi\": \"maori\",\n",
        "    \"ml\": \"malayalam\",\n",
        "    \"cy\": \"welsh\",\n",
        "    \"sk\": \"slovak\",\n",
        "    \"te\": \"telugu\",\n",
        "    \"fa\": \"persian\",\n",
        "    \"lv\": \"latvian\",\n",
        "    \"bn\": \"bengali\",\n",
        "    \"sr\": \"serbian\",\n",
        "    \"az\": \"azerbaijani\",\n",
        "    \"sl\": \"slovenian\",\n",
        "    \"kn\": \"kannada\",\n",
        "    \"et\": \"estonian\",\n",
        "    \"mk\": \"macedonian\",\n",
        "    \"br\": \"breton\",\n",
        "    \"eu\": \"basque\",\n",
        "    \"is\": \"icelandic\",\n",
        "    \"hy\": \"armenian\",\n",
        "    \"ne\": \"nepali\",\n",
        "    \"mn\": \"mongolian\",\n",
        "    \"bs\": \"bosnian\",\n",
        "    \"kk\": \"kazakh\",\n",
        "    \"sq\": \"albanian\",\n",
        "    \"sw\": \"swahili\",\n",
        "    \"gl\": \"galician\",\n",
        "    \"mr\": \"marathi\",\n",
        "    \"pa\": \"punjabi\",\n",
        "    \"si\": \"sinhala\",\n",
        "    \"km\": \"khmer\",\n",
        "    \"sn\": \"shona\",\n",
        "    \"yo\": \"yoruba\",\n",
        "    \"so\": \"somali\",\n",
        "    \"af\": \"afrikaans\",\n",
        "    \"oc\": \"occitan\",\n",
        "    \"ka\": \"georgian\",\n",
        "    \"be\": \"belarusian\",\n",
        "    \"tg\": \"tajik\",\n",
        "    \"sd\": \"sindhi\",\n",
        "    \"gu\": \"gujarati\",\n",
        "    \"am\": \"amharic\",\n",
        "    \"yi\": \"yiddish\",\n",
        "    \"lo\": \"lao\",\n",
        "    \"uz\": \"uzbek\",\n",
        "    \"fo\": \"faroese\",\n",
        "    \"ht\": \"haitian creole\",\n",
        "    \"ps\": \"pashto\",\n",
        "    \"tk\": \"turkmen\",\n",
        "    \"nn\": \"nynorsk\",\n",
        "    \"mt\": \"maltese\",\n",
        "    \"sa\": \"sanskrit\",\n",
        "    \"lb\": \"luxembourgish\",\n",
        "    \"my\": \"myanmar\",\n",
        "    \"bo\": \"tibetan\",\n",
        "    \"tl\": \"tagalog\",\n",
        "    \"mg\": \"malagasy\",\n",
        "    \"as\": \"assamese\",\n",
        "    \"tt\": \"tatar\",\n",
        "    \"haw\": \"hawaiian\",\n",
        "    \"ln\": \"lingala\",\n",
        "    \"ha\": \"hausa\",\n",
        "    \"ba\": \"bashkir\",\n",
        "    \"jw\": \"javanese\",\n",
        "    \"su\": \"sundanese\",\n",
        "}"
      ],
      "metadata": {
        "id": "LJU0X5QJ5TWa"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ref: https://github.com/openai/gpt-2/blob/master/src/encoder.py\n",
        "def bytes_to_unicode():\n",
        "    \"\"\"\n",
        "    Returns list of utf-8 byte and a corresponding list of unicode strings.\n",
        "    The reversible bpe codes work on unicode strings.\n",
        "    This means you need a large # of unicode characters in your vocab if you want to avoid UNKs.\n",
        "    When you're at something like a 10B token dataset you end up needing around 5K for decent coverage.\n",
        "    This is a signficant percentage of your normal, say, 32K bpe vocab.\n",
        "    To avoid that, we want lookup tables between utf-8 bytes and unicode strings.\n",
        "    And avoids mapping to whitespace/control characters the bpe code barfs on.\n",
        "    \"\"\"\n",
        "    bs = list(range(ord(\"!\"), ord(\"~\")+1))+list(range(ord(\"¡\"), ord(\"¬\")+1))+list(range(ord(\"®\"), ord(\"ÿ\")+1))\n",
        "    cs = bs[:]\n",
        "    n = 0\n",
        "    for b in range(2**8):\n",
        "        if b not in bs:\n",
        "            bs.append(b)\n",
        "            cs.append(2**8+n)\n",
        "            n += 1\n",
        "    cs = [chr(n) for n in cs]\n",
        "    return dict(zip(bs, cs))"
      ],
      "metadata": {
        "id": "ubnuRn2475bg"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Generate filters and vocab binary"
      ],
      "metadata": {
        "id": "rqPLYlya9lE9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import io\n",
        "import os\n",
        "import sys\n",
        "import struct\n",
        "import json\n",
        "import code\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "fname_inp = \"/content/tiny.en.pt\"\n",
        "# try to load PyTorch binary data\n",
        "try:\n",
        "    model_bytes = open(fname_inp, \"rb\").read()\n",
        "    with io.BytesIO(model_bytes) as fp:\n",
        "        checkpoint = torch.load(fp, map_location=\"cpu\")\n",
        "except:\n",
        "    print(\"Error: failed to load PyTorch model file: %s\" % fname_inp)\n",
        "    sys.exit(1)\n",
        "\n",
        "whisperparams = checkpoint[\"dims\"]\n",
        "print(\"whisperparams:\", whisperparams)\n",
        "list_vars = checkpoint[\"model_state_dict\"]\n",
        "#print(list_vars['encoder.positional_embedding'])\n",
        "#print(list_vars['encoder.conv1.weight'])\n",
        "#print(list_vars['encoder.conv1.weight'].shape)\n",
        "\n",
        "# load mel filters\n",
        "n_mels = whisperparams[\"n_mels\"]\n",
        "with np.load(os.path.join(\"/content/whisper/whisper/assets\", \"mel_filters.npz\")) as f:\n",
        "    filters = torch.from_numpy(f[f\"mel_{n_mels}\"])\n",
        "    #print (filters)\n",
        "\n",
        "multilingual = whisperparams[\"n_vocab\"] == 51865\n",
        "tokenizer = build_tokenizer(\"/content/whisper/\", multilingual and \"multilingual\" or \"gpt2\")\n",
        "\n",
        "#print(tokenizer)\n",
        "#print(tokenizer.name_or_path)\n",
        "#print(len(tokenizer.additional_special_tokens))\n",
        "dir_tokenizer = tokenizer.name_or_path\n",
        "print(\"dir_tokenizer:\",dir_tokenizer)\n",
        "\n",
        "# output in the same directory as the model\n",
        "fname_out = \"./filters_vocab_gen.bin\"\n",
        "with open(dir_tokenizer + \"/vocab.json\", \"r\", encoding=\"utf8\") as f:\n",
        "    tokens = json.load(f)\n",
        "fout = open(fname_out, \"wb\")\n",
        "fout.write(struct.pack(\"i\", 0x5553454e)) # magic: USEN in hex    \n",
        "#fout.write(struct.pack(\"i\", whisperparams[\"n_vocab\"]))\n",
        "#print(\"n_vocab:\",whisperparams[\"n_vocab\"])\n",
        "#fout.write(struct.pack(\"i\", whisperparams[\"n_mels\"]))\n",
        "#print(\"n_mels:\",n_mels)\n",
        "# write mel filters\n",
        "fout.write(struct.pack(\"i\", filters.shape[0]))\n",
        "print(\"filters.shape[0]:\",filters.shape[0])\n",
        "fout.write(struct.pack(\"i\", filters.shape[1]))\n",
        "print(\"filters.shape[0]:\",filters.shape[1])\n",
        "for i in range(filters.shape[0]):\n",
        "    for j in range(filters.shape[1]):\n",
        "        fout.write(struct.pack(\"f\", filters[i][j]))\n",
        "byte_encoder = bytes_to_unicode()\n",
        "byte_decoder = {v:k for k, v in byte_encoder.items()}\n",
        "\n",
        "fout.write(struct.pack(\"i\", len(tokens)))\n",
        "print(\"len(tokens):\",len(tokens))\n",
        "for key in tokens:\n",
        "    text = bytearray([byte_decoder[c] for c in key])\n",
        "    fout.write(struct.pack(\"i\", len(text)))\n",
        "    fout.write(text)\n",
        "fout.close()\n",
        "\n",
        "print(\"Done. Output file: \" + fname_out)\n",
        "print(\"\")    \n",
        "        "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A6xl0Bn30J08",
        "outputId": "1b5d5b3a-394e-4dd1-f9ed-cd082ab9ddca"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "whisperparams: {'n_mels': 80, 'n_vocab': 51864, 'n_audio_ctx': 1500, 'n_audio_state': 384, 'n_audio_head': 6, 'n_audio_layer': 4, 'n_text_ctx': 448, 'n_text_state': 384, 'n_text_head': 6, 'n_text_layer': 4}\n",
            "dir_tokenizer: /content/whisper/whisper/assets/gpt2\n",
            "filters.shape[0]: 80\n",
            "filters.shape[0]: 201\n",
            "len(tokens): 50257\n",
            "Done. Output file: ./filters_vocab_gen.bin\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "z0fUzw8M8838"
      }
    }
  ]
}
