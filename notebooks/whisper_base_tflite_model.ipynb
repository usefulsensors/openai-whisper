{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##Install required tools"
      ],
      "metadata": {
        "id": "qeVMElVTkeVo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Installing whisper package\n",
        "!pip install git+https://github.com/openai/whisper.git\n",
        "\n",
        "#Insalloing datasets\n",
        "!pip install datasets\n",
        "\n",
        "# Cloning whisper repository \n",
        "!git clone https://github.com/openai/whisper.git\n",
        "\n",
        "# Downloading a sample audio file \n",
        "!wget https://huggingface.co/datasets/osanseviero/dummy_ja_audio/resolve/main/result.flac"
      ],
      "metadata": {
        "id": "lmSS-RSdjXhp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Generate Whisper TFLite model"
      ],
      "metadata": {
        "id": "iGP8xDa3knH2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "Vb9TvLmehMQa",
        "outputId": "64b0d7cf-efb6-4118-aec8-2fb24883676d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFWhisperForConditionalGeneration.\n",
            "\n",
            "All the layers of TFWhisperForConditionalGeneration were initialized from the model checkpoint at openai/whisper-base.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFWhisperForConditionalGeneration for predictions without further training.\n",
            "WARNING:datasets.builder:Found cached dataset librispeech_asr_dummy (/root/.cache/huggingface/datasets/hf-internal-testing___librispeech_asr_dummy/clean/2.1.0/d3bc4c2bc2078fcde3ad0f0f635862e4c0fef78ba94c4a34c4c250a097af240b)\n",
            "It is strongly recommended to pass the `sampling_rate` argument to this function. Failing to do so can result in silent errors that might be hard to debug.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Mr. Quilter is the apostle of the middle classes, and we are\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/impl/api.py:371: UserWarning: Neither `max_length` nor `max_new_tokens` have been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
            "  return py_builtins.overload_of(f)(*args)\n",
            "WARNING:absl:Found untraced functions such as encoder_layer_call_fn, encoder_layer_call_and_return_conditional_losses, decoder_layer_call_fn, decoder_layer_call_and_return_conditional_losses, conv1_layer_call_fn while saving (showing 5 of 420). These functions will not be directly callable after loading.\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "# Importing necessary classes from transformers \n",
        "from transformers import AutoProcessor, TFWhisperForConditionalGeneration, GenerationConfig\n",
        "\n",
        "# Importing necessary functions from datasets\n",
        "from datasets import load_dataset\n",
        "\n",
        "\n",
        "# Creating force_token_map to be used in GenerationConfig\n",
        "force_token_map = [[50258, 50266], [50359, 50363]] #\n",
        "\n",
        "# Creating generation_config with force_token_map\n",
        "generation_config = GenerationConfig(force_token_map=force_token_map)\n",
        "\n",
        "# Creating an instance of AutoProcessor from the pretrained model\n",
        "processor = AutoProcessor.from_pretrained(\"openai/whisper-base\")\n",
        "\n",
        "# Creating an instance of TFWhisperForConditionalGeneration from the pretrained model\n",
        "model = TFWhisperForConditionalGeneration.from_pretrained(\"openai/whisper-base\")\n",
        "\n",
        "# Loading dataset\n",
        "ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n",
        "\n",
        "# Inputs\n",
        "inputs = processor(ds[0][\"audio\"][\"array\"], return_tensors=\"tf\")\n",
        "input_features = inputs.input_features\n",
        "\n",
        "# Generating Transcription\n",
        "generated_ids = model.generate(input_ids=input_features, generation_config=generation_config)\n",
        "transcription = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
        "print(transcription)\n",
        "\n",
        "# Creating a GenerateModel Class\n",
        "class GenerateModel(tf.Module):\n",
        "  def __init__(self, model):\n",
        "    super(GenerateModel, self).__init__()\n",
        "    self.model = model\n",
        "\n",
        "  @tf.function(\n",
        "    input_signature=[\n",
        "      tf.TensorSpec(shape=(1, 80,3000), dtype=tf.float32, name=\"input_ids\"),\n",
        "    ]\n",
        "  )\n",
        "  def serving(self, input_ids):\n",
        "    outputs = self.model.generate(input_ids, forced_decoder_ids=force_token_map)\n",
        "    return {\"sequences\": outputs}\n",
        "\n",
        "# Saving the model\n",
        "saved_model_dir = '/content/tf'\n",
        "generate_model = GenerateModel(model=model)\n",
        "tf.saved_model.save(generate_model, saved_model_dir, signatures={\"serving_default\": generate_model.serving})\n",
        "\n",
        "# Converting to TFLite model\n",
        "tflite_model_path = '/content/whisper-base.tflite'\n",
        "converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "# Saving the TFLite model\n",
        "with open(tflite_model_path, 'wb') as f:\n",
        "    f.write(tflite_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Run the inference on Whisper TFLite model"
      ],
      "metadata": {
        "id": "N2b4VZBqkpT9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import whisper\n",
        "import numpy as np\n",
        "from timeit import default_timer as timer\n",
        "\n",
        "\n",
        "# Define the path to the TFLite model\n",
        "tflite_model_path = '/content/whisper-base.tflite'\n",
        "\n",
        "# Create an interpreter to run the TFLite model\n",
        "interpreter = tf.lite.Interpreter(tflite_model_path)\n",
        "\n",
        "# Allocate memory for the interpreter\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "# Get the input and output tensors\n",
        "input_tensor = interpreter.get_input_details()[0]['index']\n",
        "output_tensor = interpreter.get_output_details()[0]['index']\n",
        "\n",
        "\n",
        "inference_start = timer()\n",
        "\n",
        "# Calculate the mel spectrogram of the audio file\n",
        "print(f'Calculating mel spectrogram...')\n",
        "mel_from_file = whisper.audio.log_mel_spectrogram('/content/whisper/tests/jfk.flac')\n",
        "\n",
        "# Pad or trim the input data to match the expected input size\n",
        "input_data = whisper.audio.pad_or_trim(mel_from_file, whisper.audio.N_FRAMES)\n",
        "\n",
        "# Add a batch dimension to the input data\n",
        "input_data = np.expand_dims(input_data, 0)\n",
        "\n",
        "# Run the TFLite model using the interpreter\n",
        "print(\"Invoking interpreter ...\")\n",
        "interpreter.set_tensor(input_tensor, input_data)\n",
        "interpreter.invoke()\n",
        "\n",
        "# Get the output data from the interpreter\n",
        "output_data = interpreter.get_tensor(output_tensor)\n",
        "\n",
        "# Print the output data\n",
        "#print(output_data)\n",
        "\n",
        "# Create a tokenizer to convert tokens to text\n",
        "wtokenizer = whisper.tokenizer.get_tokenizer(True, language=\"ja\")\n",
        "\n",
        "# convert tokens to text\n",
        "print(\"Converting tokens ...\")\n",
        "for token in output_data:\n",
        "    # Replace -100 with the end of text token\n",
        "    token[token == -100] = wtokenizer.eot\n",
        "    text = wtokenizer.decode(token, skip_special_tokens=True)\n",
        "    print(text)\n",
        "\n",
        "print(\"\\nInference took {:.2f}s \".format(timer() - inference_start))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TzCrY9Q5jVsg",
        "outputId": "d4f9e40f-944f-4435-fbb2-a70d98325e02"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calculating mel spectrogram...\n",
            "Invoking interpreter ...\n",
            "Converting tokens ...\n",
            " And so my fellow Americans, ask not what your country can do for you, ask what you can do for your country.\n",
            "\n",
            "Inference took 12.02s \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%ls -la"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3cZE5TG5loVZ",
        "outputId": "23e24465-4cd8-448a-81ee-2d3d7beffe7c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 122808\n",
            "drwxr-xr-x 1 root root      4096 Jan 26 00:57 \u001b[0m\u001b[01;34m.\u001b[0m/\n",
            "drwxr-xr-x 1 root root      4096 Jan 26 00:47 \u001b[01;34m..\u001b[0m/\n",
            "drwxr-xr-x 4 root root      4096 Jan 24 14:37 \u001b[01;34m.config\u001b[0m/\n",
            "-rw-r--r-- 1 root root     70166 Oct  7 14:23 result.flac\n",
            "-rw-r--r-- 1 root root     70166 Oct  7 14:23 result.flac.1\n",
            "drwxr-xr-x 1 root root      4096 Jan 24 14:38 \u001b[01;34msample_data\u001b[0m/\n",
            "drwxr-xr-x 4 root root      4096 Jan 26 00:56 \u001b[01;34mtf\u001b[0m/\n",
            "drwxr-xr-x 8 root root      4096 Jan 26 00:48 \u001b[01;34mwhisper\u001b[0m/\n",
            "-rw-r--r-- 1 root root 125580904 Jan 26 00:57 whisper-base.tflite\n"
          ]
        }
      ]
    }
  ]
}